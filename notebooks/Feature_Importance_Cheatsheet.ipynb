{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ooRiHFY5Fz-OxESMtjJh5M7X1qv-55li","timestamp":1757000415554},{"file_id":"1nF85w34C2X4teAjyUUnMo6QD_xugdPNR","timestamp":1756893449242}],"authorship_tag":"ABX9TyPzuNCGzrfRPRlyAuwtRgH+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Feature Importance Cheatsheet"],"metadata":{"id":"aZXs6qJRkSrX"}},{"cell_type":"markdown","source":["## Feature of Importance from Embedded Models\n","\n","Embedded feature selection methods determine feature importance as part of the model training process, unlike filter or wrapper methods. They leverage the internal structure of a model to identify the most informative features, making selection efficient and model-aware.\n","\n","* Tree-based models (e.g., XGBoost, Random Forest, CatBoost): Feature importance is derived from splits (gain, cover, or frequency)\n","* Linear models (e.g., Lasso, Ridge): Importance is based on the magnitude of coefficients.\n","\n","For XGBoost specifically, feature importance can be measured in several ways:\n","* **weight** counts how many times a feature is used to split the data across all trees\n","* **gain** measures the average improvement in loss (e.g., reduction in error) brought by splits on that feature.\n","* **cover** reflects the relative number of samples affected by the splits involving that feature.\n"],"metadata":{"id":"XNe7ZrSPm8hd"}},{"cell_type":"code","source":["# Reinstantiate Model with all features.\n","model = xgb.XGBClassifier(\n","    n_estimators=500,\n","    eval_metric='logloss',\n","    random_state=42,\n","    seed=42\n",")\n","model.fit(X_train, y_train)\n","\n","importance_types = ['weight', 'gain', 'total_gain', 'cover', 'total_cover']\n","\n","fig, axes = plt.subplots(1, len(importance_types), figsize=(5*len(importance_types), 6))\n","\n","for ax, imp_type in zip(axes, importance_types):\n","    xgb.plot_importance(\n","        model,\n","        importance_type=imp_type,\n","        height=0.4,\n","        ax=ax,\n","        color='steelblue'\n","    )\n","    ax.set_title(f'Feature Importance ({imp_type})')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"gYHCt19yrTo4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using methods like SelectFromModel, you can automatically select features above a certain importance threshold.\n","\n","Key considerations:\n","* Model-dependent: Different models rank features differently due to their internal mechanics.\n","* Training-dependent: Hyperparameters, randomness, and feature correlations can affect importance scores.\n","* Interpretation: Embedded importance reflects which features are most informative for the trained model, not necessarily universally important."],"metadata":{"id":"bNyJH5IFrW-X"}},{"cell_type":"code","source":["# Reinstantiate Model with all features.\n","model = xgb.XGBClassifier(\n","    n_estimators=500,\n","    eval_metric='logloss',\n","    random_state=42,\n","    seed=42\n",")\n","model.fit(X_train[feature_cols], y_train)\n","\n","# model.set_params(importance_type='cover') # You can use this line to determine which importance type to use.\n","\n","embedded_selector = SelectFromModel(model, prefit=True)\n","selected_features = X_train[feature_cols].columns[embedded_selector.get_support()]\n","print(\"Embedded selected features:\", selected_features)\n"],"metadata":{"id":"ACJAEcs6nA45"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Feature Importance from SHAP (SHapley Additive exPlanations)\n","\n","SHAP is a model-agnostic explainability method that quantifies the contribution of each feature to a model’s prediction. It is based on Shapley values from cooperative game theory, which fairly distribute the “payout” (prediction) among all features according to their contribution.\n","\n","How it works:\n","\n","1. For a given prediction, SHAP considers all possible combinations of features and measures the marginal contribution of each feature when added to a subset.\n","2. These contributions are averaged across all permutations to compute a Shapley value for each feature.\n","3. The sum of all feature Shapley values plus the expected model output equals the model’s prediction for that instance.\n","\n","**TreeExplainer**: For tree-based models like XGBoost, shap.TreeExplainer efficiently computes exact Shapley values using the tree structure, making it fast and scalable."],"metadata":{"id":"Mk0JfmgcL_Ha"}},{"cell_type":"code","source":["# Reinstantiate Model with all features.\n","model = xgb.XGBClassifier(\n","    n_estimators=500,\n","    eval_metric='logloss',\n","    random_state=42,\n","    seed=42\n",")\n","model.fit(X_train[feature_cols], y_train)\n","\n","explainer = shap.TreeExplainer(model)\n","shap_values = explainer.shap_values(X_val[feature_cols])\n","shap.summary_plot(shap_values, X_val[feature_cols], feature_names=feature_cols)"],"metadata":{"id":"zi7sNOLDLLi3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Permutation Feature Importance\n","\n","Permutation feature importance is a post-hoc, model-agnostic method for evaluating the impact of each feature on a trained model’s performance. Unlike embedded methods that rely on the model’s internal feature scoring, permutation importance directly measures how much the model’s predictive ability decreases when a feature’s values are randomly shuffled.\n","\n","How it works:\n","\n","* Take a trained model and a validation dataset.\n","* Compute a baseline performance metric (e.g., recall, AUC) on the validation set.\n","* For each feature, randomly permute its values across all samples, breaking any association with the target.\n","* Measure the drop in model performance caused by the permutation.\n","\n","Features that cause a larger drop are deemed more important."],"metadata":{"id":"xvbTP4oXnLKD"}},{"cell_type":"code","source":["# Reinstantiate Model with all features.\n","model = xgb.XGBClassifier(\n","    n_estimators=500,\n","    eval_metric='logloss',\n","    random_state=42,\n","    seed=42\n",")\n","model.fit(X_train[feature_cols], y_train)\n","\n","def auc_scorer(estimator, X, y):\n","    y_proba = estimator.predict_proba(X)[:, 1]\n","    return roc_auc_score(y, y_proba)\n","\n","def f1_scorer(estimator, X, y):\n","    y_pred = estimator.predict(X)\n","    return f1_score(y, y_pred)\n","\n","def accuracy_scorer(estimator, X, y):\n","    y_pred = estimator.predict(X)\n","    return accuracy_score(y, y_pred)\n","\n","def precision_scorer(estimator, X, y):\n","    y_pred = estimator.predict(X)\n","    return precision_score(y, y_pred)\n","\n","def recall_scorer(estimator, X, y):\n","    y_pred = estimator.predict(X)\n","    return recall_score(y, y_pred)\n","\n","perm_importance = permutation_importance(model,\n","                                         X_val,\n","                                         y_val,\n","                                         n_repeats=10,\n","                                         random_state=42,\n","                                         scoring=auc_scorer # This can be changed with different scorers.\n","                                         )\n","\n","sorted_idx = perm_importance.importances_mean.argsort()[::-1]\n","print(\"Top features by permutation importance:\")\n","for idx in sorted_idx[:10]:\n","    print(X_val.columns[idx], perm_importance.importances_mean[idx])\n"],"metadata":{"id":"rD53_nG7nMzp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Iterative Wrapper Methods\n","\n","This is a list of methods which uses either embedded feature importance OR metrics on the model's outputs to identify features.\n","\n","1. Recursive Feature Elimination & Boruta Algorithm: Uses the models own embedded feature importance. (_feature_importance or _coeff (for logistic regression models))\n","2. Sequential Feature Selection: Tunes the features on a specific metric calculated from the model's outputs."],"metadata":{"id":"rfqAa6LCm4hz"}},{"cell_type":"markdown","source":["### Recursive Feature Elimination (RFE)\n","\n","Recursive Feature Elimination (RFE) is a wrapper-based feature selection method that iteratively selects the most important features for a predictive model. The basic idea is:\n","\n","1. Start with all features and train the estimator (in this case, XGBClassifier).\n","2. Compute feature importance scores from the trained model.\n","3. Remove the least important feature(s).\n","4. Refit the model on the remaining features.\n","5. Repeat this process until the desired number of features (n_features_to_select) is reached.\n","\n","RFE is particularly useful when you want to systematically reduce the feature space while retaining the most informative predictors."],"metadata":{"id":"2DXAKHbzCOzk"}},{"cell_type":"code","source":["rfe_selector = RFE(\n","    estimator=xgb.XGBClassifier(n_estimators=100,\n","                                random_state=42,\n","                                importance_type=\"gain\" # This can be changed to \"cover\" etc.\n","                                ),\n","    n_features_to_select=5\n",")\n","rfe_selector.fit(X_train[feature_cols], y_train)\n","print(\"RFE selected features:\", [feature_cols[i] for i in rfe_selector.get_support(indices=True)])"],"metadata":{"id":"NDvZu25GCLQj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sequential Feature Selection (SFS)\n","\n","This is a wrapper-based method for selecting a subset of features by sequentially adding or removing features based on their contribution to model performance. There are two main approaches:\n","\n","Forward selection (direction='forward'):\n","\n","1. Starts with no features.\n","2. Iteratively adds the feature that improves performance the most.\n","3. Stops when the desired number of features (n_features_to_select) is reached.\n","\n","Backward elimination (direction='backward'):\n","\n","1. Starts with all features.\n","2. Iteratively removes the least important feature.\n","3. Stops when the desired number of features remains."],"metadata":{"id":"ZMoFnAObCX4x"}},{"cell_type":"code","source":["def auc_scorer(estimator, X, y):\n","    y_proba = estimator.predict_proba(X)[:, 1]\n","    return roc_auc_score(y, y_proba)\n","\n","def f1_scorer(estimator, X, y):\n","    y_pred = estimator.predict(X)\n","    return f1_score(y, y_pred)\n","\n","def accuracy_scorer(estimator, X, y):\n","    y_pred = estimator.predict(X)\n","    return accuracy_score(y, y_pred)\n","\n","def precision_scorer(estimator, X, y):\n","    y_pred = estimator.predict(X)\n","    return precision_score(y, y_pred)\n","\n","def recall_scorer(estimator, X, y):\n","    y_pred = estimator.predict(X)\n","    return recall_score(y, y_pred)\n","\n","sfs_selector = SequentialFeatureSelector(\n","    xgb.XGBClassifier(n_estimators=100,\n","                      random_state=42\n","                      ),\n","    n_features_to_select=5,\n","    direction='forward', # This could also be \"backward\"\n","    scoring=auc_scorer # This can be changed with the other scorers\n",")\n","sfs_selector.fit(X_train[feature_cols], y_train)\n","print(\"SFS selected features:\", [feature_cols[i] for i in sfs_selector.get_support(indices=True)])"],"metadata":{"id":"PG2lKLswCa6d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Boruta Algorithm\n","\n","Boruta is a wrapper-based feature selection algorithm that aims to identify all relevant features for a predictive model, rather than just a minimal subset. It works by comparing the importance of real features against **“shadow” features**.\n","\n","Shadow features are synthetic copies of your original features, created by randomly shuffling the values of each feature across all samples. This preserves the distribution of the original feature but breaks any real association with the target variable. Features that consistently outperform their shadow counterparts are considered important and retained.\n","\n","The selected_features list contains all features that Boruta deemed truly relevant for predicting the target variable. This method is particularly useful when you want a comprehensive set of predictive features, including those that might be missed by simpler methods like RFE or sequential selection.\n","\n","Key points:\n","\n","* Boruta identifies all relevant features, not just the top N.\n","* Works well with tree-based models like Random Forest, XGBoost, or ExtraTrees because they provide feature importance scores.\n","* Reduces the risk of omitting important features when building the final predictive model.\n","\n","There is a variation of buruta that uses SHAP values, but it takes a long time to run. If you would like to read more on this, the working package of this is called **\"borutashapplus\".**"],"metadata":{"id":"tZSX-aSeCkMu"}},{"cell_type":"code","source":["boruta_selector = BorutaPy(\n","    estimator=xgb.XGBClassifier(n_estimators=100,\n","                                random_state=42,\n","                                importance_type=\"gain\" # This can be changed to \"cover\" etc.\n","                                ),\n","    n_estimators='auto',\n","    verbose=0,\n","    random_state=42\n",")\n","\n","boruta_selector.fit(X_train.fillna(0).values, y_train.values)\n","\n","selected_features = [feature_cols[i] for i in range(len(feature_cols)) if boruta_selector.support_[i]]\n","print(\"Boruta selected features:\", selected_features)\n"],"metadata":{"id":"a-e9VHOWmeMM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Statistical Filter Methods\n","\n","This provides guidance on a range of statistical based methods which can also be used for feature selection.\n","\n","Have a go at running these tests, to see how it compares with your feature importance runs."],"metadata":{"id":"MqQmMThHmzkD"}},{"cell_type":"markdown","source":["### ANOVA F-test\n","This is a statistical method used to identify features that are most strongly related to the target variable. It is commonly applied when the target is categorical (e.g., classes 0 and 1) and the features are numeric.\n","\n","**How it works:**\n","\n","1. For each feature, the F-test compares the variance between groups (classes) to the variance within groups.\n","2. A higher F-score indicates that the feature’s values differ significantly across classes — meaning it is more informative for predicting the target.\n","3. SelectKBest(score_func=f_classif, k=5) selects the top 5 features with the highest F-scores."],"metadata":{"id":"I5VVGS7qAXtZ"}},{"cell_type":"code","source":["anova_selector = SelectKBest(score_func=f_classif, k=5)\n","anova_selector.fit(X_train[feature_cols], y_train) # This is currently using all features, but you will need to use numeric features\n","\n","print(\"ANOVA selected features:\", [feature_cols[i] for i in anova_selector.get_support(indices=True)])"],"metadata":{"id":"kCzozP9bc7Uc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Chi-squared Test\n","\n","This is a statistical method used to identify features that are most strongly associated with a categorical target variable. It is commonly applied when both the target and the features are categorical, such as when the feature is binary (0/1) and the target is a class label.\n","\n","How it works:\n","\n","1. For each feature, the Chi-squared test compares the observed frequency of each feature value in each class to the frequency expected if there were no association.\n","\n","2. A higher Chi-squared statistic indicates that the feature’s distribution differs significantly across classes — meaning it is more informative for predicting the target.\n","\n","3. SelectKBest(score_func=chi2, k=5) can be used to select the top 5 features with the highest Chi-squared scores."],"metadata":{"id":"E5R1DsWs38cZ"}},{"cell_type":"code","source":["chi2_selector = SelectKBest(score_func=chi2, k=5)\n","chi2_selector.fit(X_train[feature_cols], y_train) # This is currently using all features, but you will need to use binary features\n","\n","print(\"Chi2 selected features:\", [feature_cols[i] for i in chi2_selector.get_support(indices=True)])"],"metadata":{"id":"OupJ9itl4GM4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Mutual Information\n","\n","Mutual Information (MI) measures the dependency between each feature and the target variable. It captures any kind of relationship, including non-linear associations, unlike ANOVA which only detects linear separations.\n","\n","How it works:\n","1. For each feature, MI quantifies how much knowing the feature reduces uncertainty about the target.\n","2. A higher MI score means the feature contains more information about the target.\n","3. SelectKBest(score_func=mutual_info_classif, k=5) selects the top 5 features with the highest MI scores.\n","\n","By default, mutual_info_classif will treat integer columns as discrete, float columns as continuous. So make sure your binary features are either int or bool dtype to be interpreted correctly."],"metadata":{"id":"AsCdYIupBIeR"}},{"cell_type":"code","source":["mi_selector = SelectKBest(score_func=mutual_info_classif, k=5)\n","mi_selector.fit(X_train[feature_cols], y_train)\n","\n","print(\"Mutual Information features:\", [feature_cols[i] for i in mi_selector.get_support(indices=True)])"],"metadata":{"id":"rEpi0wayBHbp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MRMR (Minimum Redundancy Maximum Relevance) Summary\n","\n","MRMR is a filter-based feature selection method designed to choose features that are both:\n","* Highly relevant to the target (Maximum Relevance): Keep features that have strong statistical association with the target (e.g., high mutual information).\n","* Minimally redundant with each other (Minimum Redundancy): Avoid including features that are highly correlated or redundant with already selected features.\n","\n","How it works:\n","\n","1. Compute a relevance score (usually mutual information) between each feature and the target.\n","2. Compute redundancy between features (how much information is shared).\n","3. Iteratively select features that maximize relevance while minimizing redundancy with features already chosen.\n","4. Stop when the desired number of features K is selected.\n","\n","Notes:\n","* MRMR is model-agnostic.\n","* It works best with numeric features, and the target should be categorical or integer-encoded."],"metadata":{"id":"Xz5DYuJHQSWq"}},{"cell_type":"code","source":["k= 5\n","mrmr_features = mrmr_classif(X=X_train, y=y_train, K=k)\n","\n","print(\"\\nMRMR selected features:\", mrmr_features)"],"metadata":{"id":"I8WeIkgqFLMt"},"execution_count":null,"outputs":[]}]}